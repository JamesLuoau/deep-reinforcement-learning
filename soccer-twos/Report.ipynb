{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[//]: # (Image References)\n",
    "\n",
    "[image1]: https://user-images.githubusercontent.com/10624937/42135619-d90f2f28-7d12-11e8-8823-82b970a54d7e.gif \"Trained Agent\"\n",
    "\n",
    "# Project 1: Continuous Control"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[//]: # (Image References)\n",
    "\n",
    "[image1]: https://user-images.githubusercontent.com/10624937/42135623-e770e354-7d12-11e8-998d-29fc74429ca2.gif \"Trained Agent\"\n",
    "[image2]: https://user-images.githubusercontent.com/10624937/42135622-e55fb586-7d12-11e8-8a54-3c31da15a90a.gif \"Soccer\"\n",
    "\n",
    "\n",
    "# Project 3: Collaboration and Competition\n",
    "\n",
    "### Introduction\n",
    "\n",
    "For this project, you will work with the [Tennis](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Learning-Environment-Examples.md#tennis) environment.\n",
    "\n",
    "![Trained Agent][image1]\n",
    "\n",
    "In this environment, two agents control rackets to bounce a ball over a net. If an agent hits the ball over the net, it receives a reward of +0.1.  If an agent lets a ball hit the ground or hits the ball out of bounds, it receives a reward of -0.01.  Thus, the goal of each agent is to keep the ball in play.\n",
    "\n",
    "The observation space consists of 8 variables corresponding to the position and velocity of the ball and racket. Each agent receives its own, local observation.  Two continuous actions are available, corresponding to movement toward (or away from) the net, and jumping. \n",
    "\n",
    "The task is episodic, and in order to solve the environment, your agents must get an average score of +0.5 (over 100 consecutive episodes, after taking the maximum over both agents). Specifically,\n",
    "\n",
    "- After each episode, we add up the rewards that each agent received (without discounting), to get a score for each agent. This yields 2 (potentially different) scores. We then take the maximum of these 2 scores.\n",
    "- This yields a single **score** for each episode.\n",
    "\n",
    "The environment is considered solved, when the average (over 100 episodes) of those **scores** is at least +0.5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MADDPG Agent\n",
    "MADDPG agent, [maddpg_agent.py](maddpg_agent.py), implements the MADDPG algorithm from [MADDPG paper](https://arxiv.org/abs/1706.02275). \n",
    "\n",
    "* MADDPG is a multi-agent version of DDPG. It is adapted from DDPG to resolve multi-agent collaboration and competition problems. Although the Tennis environment may be able to be solved by a simpler way, for the sake of learning, MADDPG is employed and tried.\n",
    "* To train multiple agents to work in a collaborative, completitive or mixed environment, the algorithm adopts the framework of centralized training with decentralized execution. During training, we feed critics in each agent with full observations, observations from all agents plus additional information if available, so that agents have opportunities to learn collaboration or completition. At the same time, actors in agents use only own observations but receiving suggestions from their critics for collaboration or completition information. That is the centralized training part. In testing or execution time, we only use actors in agents and there is no change required as they require the same own observation as training time.\n",
    "* Actor and critic network parameters used in the project. The actions are included in the second layer in the critic network.\n",
    "\n",
    "    Actor network parameters\n",
    "    ```\n",
    "    ----------------------------------------------------------------\n",
    "            Layer (type)               Output Shape         Param #\n",
    "    ================================================================\n",
    "           BatchNorm1d-1                   [-1, 24]              48\n",
    "                Linear-2                  [-1, 400]          10,000\n",
    "                Linear-3                  [-1, 300]         120,300\n",
    "                Linear-4                    [-1, 2]             602\n",
    "    ================================================================\n",
    "    Total params: 130,950\n",
    "    Trainable params: 130,950\n",
    "    Non-trainable params: 0\n",
    "    ----------------------------------------------------------------\n",
    "    Input size (MB): 0.00\n",
    "    Forward/backward pass size (MB): 0.01\n",
    "    Params size (MB): 0.50\n",
    "    Estimated Total Size (MB): 0.51\n",
    "    ----------------------------------------------------------------\n",
    "    ```\n",
    "    \n",
    "    Critic network parameters\n",
    "    ```\n",
    "    ----------------------------------------------------------------\n",
    "            Layer (type)               Output Shape         Param #\n",
    "    ================================================================\n",
    "           BatchNorm1d-1                   [-1, 48]              96\n",
    "                Linear-2                  [-1, 400]          19,600\n",
    "                Linear-3                  [-1, 300]         121,500\n",
    "                Linear-4                    [-1, 1]             301\n",
    "    ================================================================\n",
    "    Total params: 141,497\n",
    "    Trainable params: 141,497\n",
    "    Non-trainable params: 0\n",
    "    ----------------------------------------------------------------\n",
    "    Input size (MB): 0.00\n",
    "    Forward/backward pass size (MB): 0.01\n",
    "    Params size (MB): 0.54\n",
    "    Estimated Total Size (MB): 0.55\n",
    "    ----------------------------------------------------------------\n",
    "    ```\n",
    "* Except for the points mentioned above, the algorithm of MADDPG is similar to DDPG. For DDPG, you can find additional information in my write-up for the [Project 2](https://github.com/weicheng113/p2_continuous-control/blob/master/Report.ipynb)\n",
    "![Algorithm](./assets/algorithm.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyper-Parameters\n",
    "* Replay buffer size, BUFFER_SIZE, of **100,000**.\n",
    "* Minibatch size, BATCH_SIZE, of **256**.\n",
    "* Discount factor, GAMMA, of **0.99**.\n",
    "* Soft update of target parameters, TAU, of **0.001**.\n",
    "* Actor learning rate of **0.0001** and critic learning rate of **0.001**.\n",
    "* Unmodified Ornstein-Uhlenbeck noise: mu of **0.0**, theta of **0.15** and sigma of **0.2**.\n",
    "* Initial noise scale: initial_noise_scale of **1.0** and noise reduction factor: noise_reduction of **0.99**. These two parameters control noise reduction in each step.\n",
    "* Episodes before training: episodes_before_train of **300**. The agent collects enough samples before starting to train."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result\n",
    "The following is training result, which is resolved in 1260 episodes with average score of 0.51 over the last 100 episodes.\n",
    "\n",
    "![Result](./assets/result.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Future Improvements\n",
    "I am interested in using PPO to resolve the problem. Unfortunately, with lots of time put in trying to train a PPO actor-critic for PongDeterministicV4, I did not finally get it working. It can learn but having memory leak issues. The issue was posted [here](https://discuss.pytorch.org/t/constant-memory-leak/28753) and the repo is [here](https://github.com/weicheng113/PongPPO). Hopefully, I can get it working in future and be able to experiment more with PPO approach.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
